import cv2
import mediapipe as mp
import numpy as np
import time
import os

# ===== ì„¤ì • =====
SAVE_DIR = r"C:\SoftwareEdu2025\project\Hand_Sound\KCH\signtotext\My data\dataset"
os.makedirs(SAVE_DIR, exist_ok=True)

POSE_SKIP_INDEXES = set(range(17, 33))  # í•˜ì²´ ì œì™¸
EXPECTED_LEN = 194
SAVE_FRAMES = 8
LABEL_NAME = "ã…•"
STABLE_SKIP_FRAMES = 4  # ì•ì˜ ë¶ˆì•ˆì •í•œ í”„ë ˆì„ ê°œìˆ˜

# ===== MediaPipe ì´ˆê¸°í™” =====
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils
holistic = mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7)

def extract_landmarks(landmarks, dims=3, skip=None):
    result = []
    if landmarks:
        for i, lm in enumerate(landmarks.landmark):
            if skip and i in skip:
                continue
            coords = [lm.x, lm.y, lm.z]
            if dims == 4:
                coords.append(getattr(lm, 'visibility', 0.0))
            result.extend(coords)
    return result

cap = cv2.VideoCapture(0)
print("â–¶ 's' í‚¤ë¥¼ ëˆŒëŸ¬ ìˆ˜ì§‘ ì‹œì‘, 'q' í‚¤ë¡œ ì¢…ë£Œ")

state = "idle"  # 'idle', 'waiting', 'collecting'
start_time = None
data_buffer = []

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        continue

    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = holistic.process(image_rgb)
    display_frame = frame.copy()

    # ëœë“œë§ˆí¬ ì‹œê°í™”
    mp_drawing.draw_landmarks(display_frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
    mp_drawing.draw_landmarks(display_frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    mp_drawing.draw_landmarks(display_frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

    current_time = time.time()

    # ìƒíƒœ ì²˜ë¦¬
    if state == "waiting":
        elapsed = current_time - start_time
        cv2.putText(display_frame, f"â±ï¸ ëŒ€ê¸° ì¤‘: {elapsed:.1f}s", (30, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
        if elapsed >= 1.0:
            state = "collecting"
            print("ğŸ“¸ ìˆ˜ì§‘ ì‹œì‘!")

    elif state == "collecting":
        lh = extract_landmarks(results.left_hand_landmarks)
        rh = extract_landmarks(results.right_hand_landmarks)
        pose = extract_landmarks(results.pose_landmarks, dims=4, skip=POSE_SKIP_INDEXES)

        keypoints = lh + rh + pose
        if len(keypoints) < EXPECTED_LEN:
            keypoints += [0.0] * (EXPECTED_LEN - len(keypoints))
        elif len(keypoints) > EXPECTED_LEN:
            keypoints = keypoints[:EXPECTED_LEN]

        data_buffer.append(keypoints)

        cv2.putText(display_frame, f"ìˆ˜ì§‘ ì¤‘: {len(data_buffer)}/{SAVE_FRAMES + STABLE_SKIP_FRAMES}", (30, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)

        if len(data_buffer) >= SAVE_FRAMES + STABLE_SKIP_FRAMES:
            output_array = np.array(data_buffer[STABLE_SKIP_FRAMES:])
            filename = f"{int(time.time())}_{LABEL_NAME}.npy"
            save_path = os.path.join(SAVE_DIR, filename)
            np.save(save_path, output_array)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
            data_buffer = []
            state = "idle"
            start_time = None

    elif state == "idle":
        cv2.putText(display_frame, "â–¶ 's' ëˆ„ë¥´ë©´ ìˆ˜ì§‘ ì‹œì‘", (30, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)

    # í‚¤ ì…ë ¥ ì²˜ë¦¬
    key = cv2.waitKey(1) & 0xFF
    if key == ord('s') and state == "idle":
        state = "waiting"
        start_time = time.time()
        data_buffer = []
        print("ğŸŸ¢ 1ì´ˆ ëŒ€ê¸° í›„ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    elif key == ord('q'):
        print("â ì¢…ë£Œ")
        break

    cv2.imshow("ì‹¤ì‹œê°„ ìˆ˜ì–´ ë°ì´í„° ìˆ˜ì§‘", display_frame)

cap.release()
cv2.destroyAllWindows()