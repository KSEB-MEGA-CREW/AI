import os
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Dropout, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from collections import defaultdict
import json
import random
import matplotlib
import matplotlib.pyplot as plt
from bayes_opt import BayesianOptimization
from scipy.interpolate import interp1d

# ===== [ì‹œê°í™” ì„¤ì •] =====
matplotlib.rcParams['font.family'] = 'Malgun Gothic'
matplotlib.rcParams['axes.unicode_minus'] = False

# ===== [ê¸°ë³¸ ì„¤ì •] =====
DATA_PATH = r"C:\SoftwareEdu2025\project\Hand_Sound\KCH\signtotext\output_npy\before"
REQUIRED_FRAMES = 12
EXPECTED_LEN = 194
MIN_VALID_FRAMES = 7
MAX_PADDING_RATIO = 0.4
SAVE_DIR = r"C:\SoftwareEdu2025\project\Hand_Sound\KCH\signtotext\train&predict\1D-CNN\models\ë³´ê°„\í…ŒìŠ¤íŠ¸"
os.makedirs(SAVE_DIR, exist_ok=True)

# ===== [ë³´ê°„ í•¨ìˆ˜] =====
def interpolate_sequence(sequence, target_len=REQUIRED_FRAMES):
    current_len = sequence.shape[0]
    if current_len == target_len:
        return sequence
    x_old = np.linspace(0, 1, num=current_len)
    x_new = np.linspace(0, 1, num=target_len)
    interpolated = interp1d(x_old, sequence, axis=0, kind='linear', fill_value="extrapolate")(x_new)
    return interpolated

# ===== [ë³´ê°„ ë””ë²„ê¹… í•¨ìˆ˜] =====
def debug_interpolation(sequence, target_len=REQUIRED_FRAMES, title=None):
    interpolated = interpolate_sequence(sequence, target_len)
    original_x = sequence[:, 0]  # ì˜ˆ: 0ë²ˆ keypointì˜ xì¢Œí‘œ
    interpolated_x = interpolated[:, 0]

    plt.figure(figsize=(10, 4))
    plt.plot(np.linspace(0, 1, len(original_x)), original_x, marker='o', label=f"ì›ë³¸ ({len(original_x)}í”„ë ˆì„)")
    plt.plot(np.linspace(0, 1, len(interpolated_x)), interpolated_x, marker='x', linestyle='--', label=f"ë³´ê°„ ({target_len}í”„ë ˆì„)")
    plt.title(title or "ë³´ê°„ ì‹œê°í™” (0ë²ˆ keypoint xì¢Œí‘œ ê¸°ì¤€)")
    plt.xlabel("ì •ê·œí™”ëœ ì‹œê°„")
    plt.ylabel("ê°’")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# ===== [ë°ì´í„° ë¡œë”© ë° ë¼ë²¨ ì²˜ë¦¬] =====
label_files = defaultdict(list)
error_files = []

for root, dirs, files in os.walk(DATA_PATH):
    for file in files:
        if file.endswith(".npy"):
            try:
                if file.startswith("none"):
                    label = "none"
                else:
                    name_split = file.split("_")
                    if len(name_split) >= 2:
                        label_part = name_split[1]
                        label = label_part.split(".")[0]
                    else:
                        error_files.append(os.path.join(root, file))
                        continue
                label_files[label].append(os.path.join(root, file))
            except Exception as e:
                print(f"âŒ íŒŒì¼ëª… íŒŒì‹± ì˜¤ë¥˜: {file} ({e})")
                error_files.append(os.path.join(root, file))

label_count_list = sorted(label_files.items(), key=lambda x: len(x[1]), reverse=True)
print("\në¼ë²¨ë³„ npy ê°œìˆ˜:")
for i, (label, files) in enumerate(label_count_list, 1):
    print(f"{i:3d}. {label:15s}: {len(files)}ê°œ")
file_counts = [len(files) for files in label_files.values()]
print(f"\nì´ ë¼ë²¨ ìˆ˜: {len(label_files)}ê°œ, ì´ npy íŒŒì¼ ìˆ˜: {sum(file_counts)}ê°œ")
if error_files:
    print(f"\n[âš ï¸ íŒŒì‹± ë¶ˆê°€ íŒŒì¼]: {error_files}")

# ===== ì‚¬ìš©ì ì…ë ¥ =====
try:
    TOP_N = int(input("\nğŸ‘‰ í•™ìŠµí•  ë¼ë²¨ ê°œìˆ˜(ì˜ˆ: 30): ").strip())
except Exception:
    TOP_N = 30
print(f"âœ… í•™ìŠµí•  ë¼ë²¨ ê°œìˆ˜: {TOP_N}")

try:
    MIN_SAMPLES = int(input("ğŸ‘‰ ë¼ë²¨ë³„ ìµœì†Œ ë°ì´í„° ê°œìˆ˜ ì´ìƒë§Œ í¬í•¨(ì˜ˆ: 30): ").strip())
except Exception:
    MIN_SAMPLES = 30
print(f"âœ… ë¼ë²¨ë³„ ìµœì†Œ ë°ì´í„° ê°œìˆ˜: {MIN_SAMPLES}")

eligible_labels = [label for label, files in label_files.items() if len(files) >= MIN_SAMPLES]
sorted_labels = sorted([(label, label_files[label]) for label in eligible_labels], key=lambda x: len(x[1]), reverse=True)
if len(eligible_labels) < TOP_N:
    raise ValueError(f"MIN_SAMPLES={MIN_SAMPLES} ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ë¼ë²¨ì´ {len(eligible_labels)}ê°œë¿ì…ë‹ˆë‹¤.")
selected_labels = [label for label, files in sorted_labels[:TOP_N]]
print(f"\n[ìµœì¢… í•™ìŠµ ë¼ë²¨ ëª©ë¡ ({TOP_N}ê°œ)]\n{selected_labels}")

# ===== [ë°ì´í„° ì „ì²˜ë¦¬ ë° ë””ë²„ê¹… ì‹œê°í™” ì¼ë¶€ í¬í•¨] =====
sequences, labels = [], []
label_dict = {label: i for i, label in enumerate(selected_labels)}

for label in selected_labels:
    files = label_files[label]
    chosen_files = random.sample(files, MIN_SAMPLES)
    label_num = label_dict[label]
    for idx, file in enumerate(chosen_files):
        sequence = np.load(file)
        if sequence.shape[0] < MIN_VALID_FRAMES:
            continue
        if idx == 0:  # ê° ë¼ë²¨ë‹¹ ì²« ë²ˆì§¸ ìƒ˜í”Œ ì‹œê°í™”
            print(f"\nğŸ“Š [ë””ë²„ê¹… ì‹œê°í™”] ë¼ë²¨: {label} ({file})")
            debug_interpolation(sequence, target_len=REQUIRED_FRAMES, title=f"{label} ë³´ê°„ ë””ë²„ê¹…")

        sequence_fixed = interpolate_sequence(sequence, REQUIRED_FRAMES)
        zero_ratio = np.sum(sequence_fixed == 0) / sequence_fixed.size
        if zero_ratio > MAX_PADDING_RATIO:
            continue
        max_abs = np.max(np.abs(sequence_fixed))
        if max_abs > 0:
            sequence_fixed = sequence_fixed / max_abs
        sequences.append(sequence_fixed)
        labels.append(label_num)

X = np.array(sequences)
y = to_categorical(labels)
print(f"\n[ìµœì¢… ë°ì´í„° shape] X={X.shape}, y={y.shape}, ë¼ë²¨ ìˆ˜={len(label_dict)}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ===== [ëª¨ë¸ ìµœì í™” ë° í•™ìŠµ] =====
def cnn_train_eval(learning_rate, dropout1, dropout2):
    from tensorflow.keras.optimizers import Adam
    model = Sequential([
        Conv1D(128, 7, activation='relu', padding='same', input_shape=(X.shape[1], X.shape[2])),
        BatchNormalization(),
        MaxPooling1D(2),
        Dropout(dropout1),
        Conv1D(256, 5, activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling1D(2),
        Dropout(dropout2),
        Flatten(),
        Dense(512, activation='relu'),
        Dropout(dropout1),
        Dense(256, activation='relu'),
        Dropout(dropout2),
        Dense(y.shape[1], activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])
    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-5, verbose=0)
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=60, batch_size=16, callbacks=[early_stop, reduce_lr], verbose=0)
    score = model.evaluate(X_test, y_test, verbose=0)
    return score[1]

pbounds = {
    'learning_rate': (1e-4, 3e-3),
    'dropout1': (0.1, 0.5),
    'dropout2': (0.1, 0.5)
}

print("\n[Bayesian Optimization] í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ì‹œì‘")
bo = BayesianOptimization(f=cnn_train_eval, pbounds=pbounds, random_state=42)
bo.maximize(init_points=5, n_iter=12)
best_params = bo.max['params']
print(f"\n[ìµœì  íŒŒë¼ë¯¸í„°] {best_params}")

# ===== [ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì €ì¥] =====
from tensorflow.keras.optimizers import Adam
model = Sequential([
    Conv1D(128, 7, activation='relu', padding='same', input_shape=(X.shape[1], X.shape[2])),
    BatchNormalization(),
    MaxPooling1D(2),
    Dropout(best_params['dropout1']),
    Conv1D(256, 5, activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling1D(2),
    Dropout(best_params['dropout2']),
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(best_params['dropout1']),
    Dense(y.shape[1], activation='softmax')
])
model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='categorical_crossentropy', metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-5, verbose=1)
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=16, callbacks=[early_stop, reduce_lr], verbose=1)

model.save(os.path.join(SAVE_DIR, "gesture_model.h5"))
label_list = [label for label, idx in sorted(label_dict.items(), key=lambda x: x[1])]
with open(os.path.join(SAVE_DIR, "label_map.json"), "w", encoding="utf-8") as f:
    json.dump(label_list, f, ensure_ascii=False)

# ===== [ì„±ëŠ¥ ì‹œê°í™” ë° í‰ê°€] =====
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„', marker='o')
plt.plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„', marker='x')
plt.title("ì •í™•ë„ ë³€í™”")
plt.xlabel("Epoch")
plt.ylabel("ì •í™•ë„")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤', marker='o')
plt.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤', marker='x')
plt.title("ì†ì‹¤ ë³€í™”")
plt.xlabel("Epoch")
plt.ylabel("ì†ì‹¤")
plt.legend()
plt.tight_layout()
plt.show()

y_pred = model.predict(X_test)
y_pred_label = np.argmax(y_pred, axis=1)
y_true_label = np.argmax(y_test, axis=1)
accuracy = np.mean(y_pred_label == y_true_label)
print(f"\n[OFFLINE TEST] ëª¨ë¸ Testì…‹ ì •í™•ë„: {accuracy:.4f}")
for i in range(min(20, len(y_true_label))):
    gt = label_list[y_true_label[i]]
    pred = label_list[y_pred_label[i]]
    print(f"[{i:02d}] ì‹¤ì œ: {gt:10s} | ì˜ˆì¸¡: {pred:10s}")