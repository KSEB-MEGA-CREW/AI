import os
import numpy as np
import json
import random
from collections import defaultdict
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Dropout, Flatten, Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from bayes_opt import BayesianOptimization

# ===== ì„¤ì • =====
DATA_PATH = r"C:\SoftwareEdu2025\project\Hand_Sound\KCH\signtotext\output_npy\before"
SAVE_DIR = r"C:\SoftwareEdu2025\project\Hand_Sound\KCH\signtotext\train&predict\1D-CNN\models\test"
os.makedirs(SAVE_DIR, exist_ok=True)

REQUIRED_FRAMES = 12
EXPECTED_LEN = 194
MIN_VALID_FRAMES = 7
MAX_PADDING_RATIO = 0.4

# ===== ë°ì´í„° ë¡œë”© ë° ì‚¬ìš©ì ì…ë ¥ =====
label_files = defaultdict(list)
error_files = []

for root, _, files in os.walk(DATA_PATH):
    for file in files:
        if file.endswith(".npy"):
            try:
                if file.startswith("none"):
                    label = "none"
                else:
                    name_split = file.split("_")
                    label = name_split[1].split(".")[0] if len(name_split) >= 2 else None
                if label:
                    label_files[label].append(os.path.join(root, file))
                else:
                    error_files.append(os.path.join(root, file))
            except Exception as e:
                print(f"âŒ íŒŒì¼ëª… íŒŒì‹± ì˜¤ë¥˜: {file} ({e})")
                error_files.append(os.path.join(root, file))

label_count_list = sorted(label_files.items(), key=lambda x: len(x[1]), reverse=True)
print("\në¼ë²¨ë³„ npy ê°œìˆ˜:")
for i, (label, files) in enumerate(label_count_list, 1):
    print(f"{i:3d}. {label:15s}: {len(files)}ê°œ")

file_counts = [len(files) for files in label_files.values()]
print(f"\nì´ ë¼ë²¨ ìˆ˜: {len(label_files)}ê°œ, ì´ npy íŒŒì¼ ìˆ˜: {sum(file_counts)}ê°œ")
if error_files:
    print(f"\n[âš ï¸ íŒŒì‹± ë¶ˆê°€ íŒŒì¼]: {error_files}")

# ===== ì‚¬ìš©ì ì…ë ¥ =====
try:
    TOP_N = int(input("\nğŸ‘‰ í•™ìŠµí•  ë¼ë²¨ ê°œìˆ˜(ì˜ˆ: 30): ").strip())
except Exception:
    TOP_N = 30
print(f"âœ… í•™ìŠµí•  ë¼ë²¨ ê°œìˆ˜: {TOP_N}")

try:
    MIN_SAMPLES = int(input("ğŸ‘‰ ë¼ë²¨ë³„ ìµœì†Œ ë°ì´í„° ê°œìˆ˜ ì´ìƒë§Œ í¬í•¨(ì˜ˆ: 30): ").strip())
except Exception:
    MIN_SAMPLES = 30
print(f"âœ… ë¼ë²¨ë³„ ìµœì†Œ ë°ì´í„° ê°œìˆ˜: {MIN_SAMPLES}")

eligible_labels = [label for label, files in label_files.items() if len(files) >= MIN_SAMPLES]
sorted_labels = sorted([(label, label_files[label]) for label in eligible_labels], key=lambda x: len(x[1]), reverse=True)

if len(eligible_labels) < TOP_N:
    raise ValueError(f"MIN_SAMPLES={MIN_SAMPLES} ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ë¼ë²¨ì´ {len(eligible_labels)}ê°œë¿ì…ë‹ˆë‹¤.")

selected_labels = [label for label, files in sorted_labels[:TOP_N]]
label_dict = {label: i for i, label in enumerate(selected_labels)}
print(f"\n[ìµœì¢… í•™ìŠµ ë¼ë²¨ ëª©ë¡ ({TOP_N}ê°œ)]\n{selected_labels}")

# ===== ë°ì´í„° ì „ì²˜ë¦¬ =====
sequences, labels = [], []

for label in selected_labels:
    files = random.sample(label_files[label], MIN_SAMPLES)
    for file in files:
        seq = np.load(file)
        if seq.shape[0] < MIN_VALID_FRAMES:
            continue
        if seq.shape[0] < REQUIRED_FRAMES:
            pad = np.zeros((REQUIRED_FRAMES - seq.shape[0], EXPECTED_LEN))
            seq = np.vstack([seq, pad])
        else:
            seq = seq[:REQUIRED_FRAMES]
        if np.sum(seq == 0) / seq.size > MAX_PADDING_RATIO:
            continue
        max_abs = np.max(np.abs(seq))
        if max_abs > 0:
            seq = seq / max_abs
        sequences.append(seq)
        labels.append(label_dict[label])

X = np.array(sequences)
y = to_categorical(labels)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"\n[ìµœì¢… ë°ì´í„° shape] X: {X.shape}, y: {y.shape}")

# ===== í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ =====
def cnn_eval(learning_rate, dropout1, dropout2):
    model = Sequential([
        Conv1D(128, 7, activation='relu', padding='same', input_shape=(REQUIRED_FRAMES, EXPECTED_LEN)),
        BatchNormalization(),
        MaxPooling1D(2),
        Dropout(dropout1),
        Conv1D(256, 5, activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling1D(2),
        Dropout(dropout2),
        Flatten(),
        Dense(512, activation='relu'),
        Dropout(dropout1),
        Dense(y.shape[1], activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=60, batch_size=16,
              validation_data=(X_test, y_test),
              callbacks=[EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),
                         ReduceLROnPlateau(monitor='val_loss', patience=4)],
              verbose=0)
    return model.evaluate(X_test, y_test, verbose=0)[1]

bo = BayesianOptimization(
    f=cnn_eval,
    pbounds={'learning_rate': (1e-4, 3e-3), 'dropout1': (0.1, 0.5), 'dropout2': (0.1, 0.5)},
    random_state=42
)
bo.maximize(init_points=5, n_iter=10)

# ===== ìµœì¢… ëª¨ë¸ í•™ìŠµ =====
best = bo.max['params']
model = Sequential([
    Conv1D(128, 7, activation='relu', padding='same', input_shape=(REQUIRED_FRAMES, EXPECTED_LEN)),
    BatchNormalization(),
    MaxPooling1D(2),
    Dropout(best['dropout1']),
    Conv1D(256, 5, activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling1D(2),
    Dropout(best['dropout2']),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(best['dropout1']),
    Dense(y.shape[1], activation='softmax')
])
model.compile(optimizer=Adam(best['learning_rate']), loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=1000, batch_size=16,
                    validation_data=(X_test, y_test),
                    callbacks=[
                        EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),
                        ReduceLROnPlateau(monitor='val_loss', patience=4, factor=0.5)
                    ])

# ===== ì €ì¥ =====
model.save(os.path.join(SAVE_DIR, "gesture_model.h5"))
label_list = [label for label, _ in sorted(label_dict.items(), key=lambda x: x[1])]
with open(os.path.join(SAVE_DIR, "label_map.json"), "w", encoding="utf-8") as f:
    json.dump(label_list, f, ensure_ascii=False)

# ===== ì‹œê°í™” =====
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„')
plt.plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„')
plt.title("ì •í™•ë„ ë³€í™”"); plt.xlabel("Epoch"); plt.ylabel("ì •í™•ë„"); plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤')
plt.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤')
plt.title("ì†ì‹¤ ë³€í™”"); plt.xlabel("Epoch"); plt.ylabel("ì†ì‹¤"); plt.legend()

plt.tight_layout()
plt.show()

# [14] ë‹¨ì¼ npy íŒŒì¼ ì˜ˆì¸¡
def predict_single_npy(npy_path):
    seq = np.load(npy_path)

    # ìœ íš¨ í”„ë ˆì„ í•„í„°ë§
    if seq.shape[0] < MIN_VALID_FRAMES:
        print("âš ï¸ í”„ë ˆì„ ìˆ˜ ë¶€ì¡±ìœ¼ë¡œ ì˜ˆì¸¡ ë¶ˆê°€")
        return

    # 0íŒ¨ë”© or ìë¥´ê¸°
    if seq.shape[0] < REQUIRED_FRAMES:
        pad = np.zeros((REQUIRED_FRAMES - seq.shape[0], EXPECTED_LEN))
        seq = np.vstack([seq, pad])
    else:
        seq = seq[:REQUIRED_FRAMES]

    # íŒ¨ë”© ë¹„ìœ¨ í™•ì¸
    if np.sum(seq == 0) / seq.size > MAX_PADDING_RATIO:
        print("âš ï¸ 0íŒ¨ë”© ë¹„ìœ¨ ê³¼ë‹¤ë¡œ ì˜ˆì¸¡ ì œì™¸")
        return

    # ì •ê·œí™”
    max_abs = np.max(np.abs(seq))
    if max_abs > 0:
        seq = seq / max_abs

    # ì°¨ì› ë§ì¶”ê¸°: (1, 12, 194)
    seq_input = np.expand_dims(seq, axis=0)

    # ì˜ˆì¸¡
    pred_probs = model.predict(seq_input)[0]
    pred_idx = np.argmax(pred_probs)
    pred_label = label_list[pred_idx]
    confidence = pred_probs[pred_idx]

    print(f"\nâœ… ì˜ˆì¸¡ ê²°ê³¼: {pred_label} (ì •í™•ë„: {confidence:.4f})")

# ğŸ” ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸: ê²½ë¡œì— ë§ëŠ” npy íŒŒì¼ ë„£ê¸°
TEST_FILE = r"C:\SoftwareEdu2025\project\Hand_Sound\KCH\signtotext\output_npy\before\example_ì±…1.npy"
predict_single_npy(TEST_FILE)