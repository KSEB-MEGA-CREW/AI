import os
import numpy as np
import json
import random
from collections import defaultdict
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight

import matplotlib

matplotlib.rcParams['font.family'] = 'Malgun Gothic'
matplotlib.rcParams['axes.unicode_minus'] = False

# ğŸ”¸ ê²½ë¡œ ì„¤ì •
DATA_PATH = r"C:\cleaned_npy"
SAVE_DIR = r"C:\SoftwareEdu2025\project\Hand_Sound\KCH\signtotext\models\final_model"
os.makedirs(SAVE_DIR, exist_ok=True)

# ğŸ”¸ ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì„¤ì •
REQUIRED_FRAMES = 10
EXPECTED_LEN = 194

# --- ë°ì´í„° ë¡œë”© (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
label_files = defaultdict(list)
for root, _, files in os.walk(DATA_PATH):
    for file in files:
        if file.endswith(".npy"):
            label = os.path.basename(root)
            label_files[label].append(os.path.join(root, file))

label_count_list = sorted(label_files.items(), key=lambda x: len(x[1]), reverse=True)
print("\nğŸ“Š ë¼ë²¨ë³„ npy ê°œìˆ˜:")
for i, (label, files) in enumerate(label_count_list, 1):
    print(f"{i:3d}. {label:15s}: {len(files)}ê°œ")

# --- [ìˆ˜ì •] ì‚¬ìš©ì ì…ë ¥ ë° ë°ì´í„° ì„ íƒ ë¡œì§ ê°œì„  ---
try:
    TOP_N = int(input("\nğŸ‘‰ í•™ìŠµí•  ë¼ë²¨ ê°œìˆ˜(ì˜ˆ: 30, 'none' ì œì™¸): ").strip())
except Exception:
    TOP_N = 30
print(f"âœ… í•™ìŠµí•  ë¼ë²¨ ê°œìˆ˜: {TOP_N}")

try:
    # [ìˆ˜ì •] ë³€ìˆ˜ëª…ì„ SAMPLES_PER_CLASSë¡œ ë³€ê²½í•˜ì—¬ ì˜ë¯¸ ëª…í™•í™”
    SAMPLES_PER_CLASS = int(input("ğŸ‘‰ ë¼ë²¨ë³„ë¡œ ì‚¬ìš©í•  ìµœëŒ€ ë°ì´í„° ê°œìˆ˜(ì˜ˆ: 320): ").strip())
except Exception:
    SAMPLES_PER_CLASS = 320
print(f"âœ… ë¼ë²¨ë³„ ìµœëŒ€ ë°ì´í„° ê°œìˆ˜: {SAMPLES_PER_CLASS}")

# 'none'ì„ ì œì™¸í•œ ë¼ë²¨ë“¤ ì¤‘ì—ì„œ ë¹ˆë„ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì„ íƒ
selected_labels = [label for label, files in label_count_list if label != "none"][:TOP_N]

# [ìˆ˜ì •] 'none' í´ë˜ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ë¬´ì¡°ê±´ í•™ìŠµì— í¬í•¨
if "none" in label_files:
    selected_labels.append("none")
    print("âœ… 'none' í´ë˜ìŠ¤ë¥¼ í•™ìŠµì— ìë™ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.")

label_dict = {label: i for i, label in enumerate(selected_labels)}
print(f"\nâœ… ìµœì¢… í•™ìŠµ ë¼ë²¨ ëª©ë¡ ({len(selected_labels)}ê°œ):\n{selected_labels}")


# --- [ìˆ˜ì •] ë°ì´í„° ì¦ê°• í•¨ìˆ˜ ì •ì˜ ---
def augment_sequence(sequence, noise_level=0.005):
    """ ë°ì´í„°ì— ì‘ì€ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì¦ê°•í•©ë‹ˆë‹¤. """
    noise = np.random.normal(0, noise_level, sequence.shape)
    return sequence + noise


# --- ë°ì´í„° ì „ì²˜ë¦¬ (ìƒ˜í”Œë§ ë°©ì‹ ë° ì¦ê°• ì ìš©) ---
sequences, labels = [], []
for label in selected_labels:
    all_files = label_files[label]

    # [ìˆ˜ì •] SAMPLES_PER_CLASSë§Œí¼ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë¡œì§ ë³€ê²½
    if len(all_files) > SAMPLES_PER_CLASS:
        files_to_use = random.sample(all_files, SAMPLES_PER_CLASS)
    else:
        files_to_use = all_files

    for file_path in files_to_use:
        seq = np.load(file_path)

        # í”„ë ˆì„ ê¸¸ì´ ë§ì¶”ê¸° (Padding / Truncating)
        if seq.shape[0] < REQUIRED_FRAMES:
            pad = np.zeros((REQUIRED_FRAMES - seq.shape[0], EXPECTED_LEN))
            seq = np.vstack([seq, pad])
        else:
            seq = seq[:REQUIRED_FRAMES]

        # ì›ë³¸ ë°ì´í„° ì¶”ê°€
        sequences.append(seq)
        labels.append(label_dict[label])

        # [ìˆ˜ì •] ë°ì´í„° ì¦ê°•: ë°ì´í„°ê°€ ë¶€ì¡±í•œ í´ë˜ìŠ¤ëŠ” 2ë°°ë¡œ ì¦ê°• (none ì œì™¸)
        if label != "none" and len(all_files) < SAMPLES_PER_CLASS:
            sequences.append(augment_sequence(seq))
            labels.append(label_dict[label])

X = np.array(sequences)
y = to_categorical(np.array(labels))

print(f"\nğŸ“ˆ ì „ì²˜ë¦¬ í›„ ì´ ë°ì´í„° ê°œìˆ˜: {len(X)}ê°œ")
print(f"   ë°ì´í„° í˜•íƒœ: {X.shape}")
print(f"   ë¼ë²¨ í˜•íƒœ: {y.shape}")

# --- í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ë°ì´í„° ë¶„í•  ---
y_indices = np.argmax(y, axis=1)
class_weights = compute_class_weight(class_weight="balanced", classes=np.unique(y_indices), y=y_indices)
class_weight_dict = {i: w for i, w in enumerate(class_weights)}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)


# --- [ìˆ˜ì •] ëª¨ë¸ êµ¬ì¡° ë³€ê²½ (LSTM ê¸°ë°˜) ---
# 1D CNNë„ ì¢‹ì§€ë§Œ, ì‹œê³„ì—´ ë°ì´í„°ì—ëŠ” LSTM/GRUê°€ ë” ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
def create_model(learning_rate=0.001, dropout_rate=0.4):
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=(REQUIRED_FRAMES, EXPECTED_LEN)),
        Dropout(dropout_rate),
        LSTM(128, return_sequences=False),
        Dropout(dropout_rate),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(dropout_rate),
        Dense(y.shape[1], activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])
    return model


# --- ëª¨ë¸ í•™ìŠµ ---
# ë² ì´ì§€ì•ˆ ìµœì í™”ëŠ” ì‹œê°„ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ, ìš°ì„  ê²€ì¦ëœ ê°’ìœ¼ë¡œ í•™ìŠµì„ ì‹œë„í•©ë‹ˆë‹¤.
model = create_model()
model.summary()

history = model.fit(X_train, y_train, epochs=200, batch_size=32,
                    validation_data=(X_test, y_test),
                    class_weight=class_weight_dict,
                    callbacks=[
                        EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True),
                        ReduceLROnPlateau(monitor='val_loss', patience=7, factor=0.5)
                    ])

# --- ëª¨ë¸ ë° ë¼ë²¨ ì €ì¥ ---
model.save(os.path.join(SAVE_DIR, "gesture_model.h5"))
label_list = [label for label, _ in sorted(label_dict.items(), key=lambda x: x[1])]
with open(os.path.join(SAVE_DIR, "label_map.json"), "w", encoding="utf-8") as f:
    json.dump(label_list, f, ensure_ascii=False)

# --- ì„±ëŠ¥ ì‹œê°í™” ë° ê²°ê³¼ í™•ì¸ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
# ... (ì´í•˜ ì‹œê°í™” ë° í…ŒìŠ¤íŠ¸ ì½”ë“œ) ...
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„')
plt.plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„')
plt.title("ì •í™•ë„ ë³€í™”");
plt.xlabel("Epoch");
plt.ylabel("ì •í™•ë„");
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤')
plt.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤')
plt.title("ì†ì‹¤ ë³€í™”");
plt.xlabel("Epoch");
plt.ylabel("ì†ì‹¤");
plt.legend()
plt.tight_layout();
plt.show()

loss, acc = model.evaluate(X_test, y_test)
print(f"\nìµœì¢… ê²€ì¦ ì •í™•ë„: {acc:.4f}")