import os
import json
import random
from collections import defaultdict
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers

# ì„ íƒ: bayes_opt ë¯¸ì„¤ì¹˜ í™˜ê²½ ëŒ€ë¹„
try:
    from bayes_opt import BayesianOptimization
    HAS_BO = True
except Exception:
    HAS_BO = False

# =============================
# ê¸°ë³¸ ì„¤ì •
# =============================
matplotlib.rcParams['font.family'] = 'Malgun Gothic'
matplotlib.rcParams['axes.unicode_minus'] = False
np.set_printoptions(suppress=True)

# ğŸ”¸ ê²½ë¡œ ì„¤ì • (í•„ìš” ì‹œ ë³€ê²½)
DATA_PATH = r"C:\\want_npy_v2"                     # ë¼ë²¨/ì‹œë‚˜ë¦¬ì˜¤ë³„ í´ë” êµ¬ì¡° í•˜ì˜ .npy
SAVE_DIR  = r"C:\\models\\v5_cnn_bo_reports[14]"   # ëª¨ë¸/ë¦¬í¬íŠ¸ ì‚°ì¶œë¬¼ ì €ì¥ í´ë”
os.makedirs(SAVE_DIR, exist_ok=True)

# ğŸ”¸ ì „ì²˜ë¦¬/ëª¨ë¸ ê¸°ë³¸ ì„¤ì •
REQUIRED_FRAMES = 10      # ì‹œí€€ìŠ¤ ê¸¸ì´(í”„ë ˆì„ ìˆ˜)
EXPECTED_LEN    = 194     # í”„ë ˆì„ë‹¹ feature ê¸¸ì´ (ì±„ë„ ìˆ˜)
TOP_N           = 17      # (none ì œì™¸) ìƒìœ„ ë¼ë²¨ ê°œìˆ˜
SAMPLES_PER_CLASS = 293   # ê° ë¼ë²¨ ìµœëŒ€ ì‚¬ìš© ìƒ˜í”Œ ìˆ˜
INCLUDE_NONE    = True    # 'none' ë¼ë²¨ ìë™ í¬í•¨
# (ì„ íƒ) noneë§Œ ë³„ë„ ìƒí•œì„ ë‘ê³  ì‹¶ìœ¼ë©´ ì•„ë˜ ê°’ì„ ë°”ê¾¸ì„¸ìš”. Noneì´ë©´ SAMPLES_PER_CLASSì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬.
SAMPLES_PER_CLASS_NONE = None

# ğŸ”¸ Bayesian Optimization ì„¤ì •
BO_ENABLE    = True
BO_INIT      = 8
BO_ITER      = 20
BO_RANDOM_SEED = 42

# ğŸ”¸ ì¬í˜„ì„± ê³ ì •
GLOBAL_SEED = 42
os.environ['PYTHONHASHSEED'] = str(GLOBAL_SEED)
random.seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)
tf.random.set_seed(GLOBAL_SEED)

# =============================
# ìœ í‹¸: ì•ˆì „í•œ ë¡œë”
# =============================
def safe_load_sequence(file_path, expected_len=EXPECTED_LEN):
    try:
        arr = np.load(file_path, allow_pickle=True)
        arr = np.asarray(arr)
        if arr.dtype == object:
            try:
                arr = np.vstack(arr)
            except Exception:
                return None, "ragged_object_array"
        if arr.ndim == 1:
            if arr.size % expected_len == 0:
                arr = arr.reshape(-1, expected_len)
            else:
                return None, f"flat_length_{arr.size}_not_multiple_of_{expected_len}"
        if arr.ndim != 2 or arr.shape[1] == 0:
            return None, f"bad_shape_{arr.shape}"
        if arr.shape[1] != expected_len:
            if arr.shape[1] > expected_len:
                arr = arr[:, :expected_len]
            else:
                pad_cols = expected_len - arr.shape[1]
                arr = np.hstack([arr, np.zeros((arr.shape[0], pad_cols), dtype=arr.dtype)])
        return arr.astype(np.float32), None
    except Exception as e:
        return None, f"load_error_{type(e).__name__}"

# =============================
# (ì¶”ê°€) ì‹œê°„ ë¦¬ìƒ˜í”Œ: ì •í™•íˆ Tí”„ë ˆì„ìœ¼ë¡œ ë³´ê°„
# =============================
def temporal_resample(seq, T=REQUIRED_FRAMES, D=EXPECTED_LEN):
    """seq:(L,D) â†’ (T,D) ì„ í˜•ë³´ê°„. L=0,1 ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨."""
    if len(seq) == 0:
        return np.zeros((T, D), dtype=np.float32)
    if len(seq) == 1:
        return np.repeat(seq.astype(np.float32), T, axis=0)

    idx = np.linspace(0, len(seq) - 1, num=T)
    i0 = np.floor(idx).astype(int)
    i1 = np.ceil(idx).astype(int)
    w = (idx - i0)[:, None].astype(np.float32)

    s0 = seq[i0].astype(np.float32)
    s1 = seq[i1].astype(np.float32)
    return (1 - w) * s0 + w * s1

# =============================
# ë°ì´í„° ë¡œë”© (ì‹œë‚˜ë¦¬ì˜¤â†’'none' ë§¤í•‘)
# =============================
# ì—¬ê¸°ì— noneìœ¼ë¡œ ì·¨ê¸‰í•  í´ë” ì´ë¦„ë“¤ì„ ë“±ë¡í•˜ì„¸ìš”.
NONE_ALIASES = {
    'none',
    'empty_frame', 'hands_down', 'typing_mouse',
    'phone_usage', 'head_touch_glasses', 'look_around'
}

label_files = defaultdict(list)
for root, _, files in os.walk(DATA_PATH):
    label_raw = os.path.basename(root)
    label = 'none' if label_raw in NONE_ALIASES else label_raw  # ğŸ”¸ í•µì‹¬: ì‹œë‚˜ë¦¬ì˜¤ë“¤ì„ 'none'ìœ¼ë¡œ í†µí•©
    for file in files:
        if file.endswith('.npy'):
            label_files[label].append(os.path.join(root, file))

# ë¼ë²¨ë³„ ê°œìˆ˜ í™•ì¸
label_count_list = sorted(label_files.items(), key=lambda x: len(x[1]), reverse=True)
print("\nğŸ“Š ë¼ë²¨ë³„ npy ê°œìˆ˜(ì‹œë‚˜ë¦¬ì˜¤ í†µí•© ë°˜ì˜):")
for i, (label, files) in enumerate(label_count_list, 1):
    print(f"{i:3d}. {label:15s}: {len(files)}ê°œ")

# í•™ìŠµ ë¼ë²¨ ëª©ë¡ êµ¬ì„±
selected_labels = [label for label, files in label_count_list if label != 'none'][:TOP_N]
if INCLUDE_NONE and 'none' in label_files:
    selected_labels.append('none')
label_dict = {label: i for i, label in enumerate(selected_labels)}
print(f"\nâœ… ìµœì¢… í•™ìŠµ ë¼ë²¨ ëª©ë¡ ({len(selected_labels)}ê°œ):\n{selected_labels}")

# (ë¯¸ë¦¬ ìƒì„±) ì¸ë±ìŠ¤â†’ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
label_list = [None]*len(selected_labels)
for name, idx in label_dict.items():
    label_list[idx] = name

# =============================
# ì¦ê°• (ì•ˆì „í•œ ë¼ë²¨ ë³´ì¡´í˜•ë§Œ)
# =============================
def augment_sequence(sequence,
                     noise_level=0.003,
                     p_time_jitter=0.5,
                     p_frame_drop=0.3):
    """
    ì–Œì „í•œ ì¦ê°•:
      1) Â±10% ì‹œê°„ ì™œê³¡ í›„ ì› ê¸¸ì´ ë³µì›
      2) ë¬´ì‘ìœ„ 1í”„ë ˆì„ ë“œë¡­ í›„ ì› ê¸¸ì´ ë³µì›
      3) ë¯¸ì„¸ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ
    """
    seq = sequence.astype(np.float32)

    # 1) ì‹œê°„ ì™œê³¡
    if np.random.rand() < p_time_jitter:
        alpha = 1.0 + np.random.uniform(-0.1, 0.1)  # Â±10%
        mid_T = max(2, int(round(len(seq) * alpha)))
        mid = temporal_resample(seq, mid_T, seq.shape[1])
        seq = temporal_resample(mid, sequence.shape[0], sequence.shape[1])

    # 2) í”„ë ˆì„ ë“œë¡­
    if len(seq) > 2 and np.random.rand() < p_frame_drop:
        drop_idx = np.random.randint(0, len(seq))
        seq2 = np.delete(seq, drop_idx, axis=0)
        seq = temporal_resample(seq2, sequence.shape[0], sequence.shape[1])

    # 3) ë¯¸ì„¸ ë…¸ì´ì¦ˆ
    seq = seq + np.random.normal(0, noise_level, seq.shape).astype(np.float32)
    return seq

# =============================
# ì „ì²˜ë¦¬ (ë¦¬ìƒ˜í”Œ â†’ 10í”„ë ˆì„ ê³ ì •)
# =============================
sequences, labels = [], []
skipped = []

# noneì˜ ìƒí•œ ê²°ì •
none_cap = SAMPLES_PER_CLASS if SAMPLES_PER_CLASS_NONE is None else SAMPLES_PER_CLASS_NONE

for label in selected_labels:
    all_files = label_files[label]
    cap_for_label = none_cap if label == 'none' else SAMPLES_PER_CLASS
    files_to_use = random.sample(all_files, min(len(all_files), cap_for_label))

    for file_path in files_to_use:
        seq, err = safe_load_sequence(file_path, expected_len=EXPECTED_LEN)
        if err is not None:
            skipped.append((file_path, err))
            continue

        # âœ… íŒ¨ë”©/ìë¥´ê¸° ëŒ€ì‹  ì‹œê°„ ë¦¬ìƒ˜í”Œë¡œ ì •í™•íˆ 10í”„ë ˆì„
        seq = temporal_resample(seq, REQUIRED_FRAMES, EXPECTED_LEN)

        sequences.append(seq)
        labels.append(label_dict[label])

        # ë°ì´í„°ê°€ ì ì€ í´ë˜ìŠ¤ì—ë§Œ ì•½í•œ ì¦ê°• 1ê°œ ì¶”ê°€ (none ì œì™¸)
        if label != 'none' and len(all_files) < SAMPLES_PER_CLASS:
            sequences.append(augment_sequence(seq))
            labels.append(label_dict[label])

X = np.array(sequences, dtype=np.float32)
y_indices = np.array(labels, dtype=np.int64)
y = to_categorical(y_indices, num_classes=len(selected_labels))

print(f"\nğŸ“ˆ ì „ì²˜ë¦¬ í›„ ì´ ë°ì´í„° ê°œìˆ˜: {len(X)}ê°œ")
print(f"   ë°ì´í„° í˜•íƒœ: {X.shape}  (frames={REQUIRED_FRAMES}, features={EXPECTED_LEN})")
print(f"   ë¼ë²¨ í˜•íƒœ: {y.shape}")

if skipped:
    print(f"\nâš ï¸ ìŠ¤í‚µí•œ íŒŒì¼: {len(skipped)}ê°œ (ì•„ë˜ 10ê°œë§Œ í‘œì‹œ)")
    for fp, why in skipped[:10]:
        print(f"- {why}: {fp}")

# =============================
# ë¼ë²¨ ë¶„í¬ ì €ì¥(ë³´ê³ ì„œìš©) - CSV/PNG
# =============================
unique, counts = np.unique(y_indices, return_counts=True)
dist_df = pd.DataFrame({
    'label_index': unique,
    'label_name': [label_list[i] for i in unique],
    'count': counts
}).sort_values('count', ascending=False)
dist_df.to_csv(os.path.join(SAVE_DIR, 'label_distribution.csv'), index=False, encoding='utf-8-sig')

plt.figure(figsize=(10, 6))
plt.bar(dist_df['label_name'], dist_df['count'])
plt.title('Label Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, 'label_distribution.png'), dpi=150)
plt.close()

# =============================
# ë°ì´í„° ë¶„í•  ë° í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
# =============================
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_indices), y=y_indices)
class_weight_dict = {i: float(w) for i, w in enumerate(class_weights)}

X_tr_all, X_te, y_tr_all, y_te, yi_tr_all, yi_te = train_test_split(
    X, y, y_indices, test_size=0.25, random_state=GLOBAL_SEED, stratify=y_indices
)
X_tr, X_va, y_tr, y_va, yi_tr, yi_va = train_test_split(
    X_tr_all, y_tr_all, yi_tr_all, test_size=0.15, random_state=GLOBAL_SEED, stratify=yi_tr_all
)

# =============================
# ëª¨ë¸: 1D-CNN
# =============================
def build_model(filters1, filters2, k1, k2, dropout, lr, l2):
    f1 = int(round(filters1))
    f2 = int(round(filters2))
    k1 = int(round(k1))
    k2 = int(round(k2))
    k1 = max(1, min(k1, REQUIRED_FRAMES))
    k2 = max(1, min(k2, REQUIRED_FRAMES))

    model = Sequential([
        Conv1D(f1, kernel_size=k1, padding='same', activation='relu',
               kernel_regularizer=regularizers.l2(l2), input_shape=(REQUIRED_FRAMES, EXPECTED_LEN)),
        BatchNormalization(),
        MaxPooling1D(pool_size=2, padding='same'),
        Dropout(dropout),

        Conv1D(f2, kernel_size=k2, padding='same', activation='relu',
               kernel_regularizer=regularizers.l2(l2)),
        BatchNormalization(),
        MaxPooling1D(pool_size=2, padding='same'),
        Dropout(dropout),

        GlobalAveragePooling1D(),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(dropout),
        Dense(y.shape[1], activation='softmax')
    ])
    model.compile(optimizer=Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# =============================
# Bayesian Optimization (ì˜µì…˜)
# =============================
best_params = {
    'filters1': 96,
    'filters2': 160,
    'k1': 3,
    'k2': 3,
    'dropout': 0.35,
    'lr': 1e-3,
    'l2': 1e-5,
    'batch_size': 32,
    'val_accuracy': None
}

if BO_ENABLE and HAS_BO:
    random.seed(BO_RANDOM_SEED); np.random.seed(BO_RANDOM_SEED); tf.random.set_seed(GLOBAL_SEED)

    def objective(filters1, filters2, k1, k2, dropout, log_lr, l2, batch_q):
        f1 = int(round(filters1)); f2 = int(round(filters2))
        kk1 = int(round(k1)); kk2 = int(round(k2))
        lr = 10 ** log_lr
        batch_choices = [16, 32, 48, 64]
        _idx = int(np.clip(np.round(batch_q), 0, len(batch_choices) - 1))
        bs = batch_choices[_idx]

        tf.random.set_seed(GLOBAL_SEED); np.random.seed(GLOBAL_SEED); random.seed(GLOBAL_SEED)

        model = build_model(f1, f2, kk1, kk2, float(dropout), float(lr), float(l2))
        cb = [
            EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_accuracy', patience=5, factor=0.5, verbose=0)
        ]
        hist = model.fit(
            X_tr, y_tr,
            validation_data=(X_va, y_va),
            epochs=100, batch_size=bs, verbose=0,
            class_weight=class_weight_dict,
            callbacks=cb
        )
        return float(np.max(hist.history['val_accuracy']))

    pbounds = {
        'filters1': (48, 192),
        'filters2': (64, 256),
        'k1': (2, 5),
        'k2': (2, 5),
        'dropout': (0.2, 0.6),
        'log_lr': (-4.0, -2.3),   # 1e-4 ~ ~5e-3
        'l2': (1e-6, 5e-4),
        'batch_q': (0, 3.99)
    }

    optimizer = BayesianOptimization(
        f=objective,
        pbounds=pbounds,
        random_state=BO_RANDOM_SEED,
        allow_duplicate_points=True
    )
    optimizer.maximize(init_points=BO_INIT, n_iter=BO_ITER)

    bp = optimizer.max['params']
    best_params['filters1'] = int(round(bp['filters1']))
    best_params['filters2'] = int(round(bp['filters2']))
    best_params['k1'] = int(round(bp['k1']))
    best_params['k2'] = int(round(bp['k2']))
    best_params['dropout'] = float(bp['dropout'])
    best_params['lr'] = float(10 ** bp['log_lr'])
    best_params['l2'] = float(bp['l2'])
    _batch_choices = [16, 32, 48, 64]
    _idx = int(np.clip(np.round(bp['batch_q']), 0, len(_batch_choices) - 1))
    best_params['batch_size'] = _batch_choices[_idx]
    best_params['val_accuracy'] = float(optimizer.max['target'])
else:
    print("\n[BO] ë¹„í™œì„±í™” ë˜ëŠ” bayes_opt ë¯¸ì„¤ì¹˜ â€” ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")

with open(os.path.join(SAVE_DIR, 'best_params.json'), 'w', encoding='utf-8') as f:
    json.dump(best_params, f, ensure_ascii=False, indent=2)
print("BEST PARAMS:", best_params)

# =============================
# ìµœì¢… í•™ìŠµ
# =============================
final_model = build_model(best_params['filters1'], best_params['filters2'], best_params['k1'], best_params['k2'],
                          best_params['dropout'], best_params['lr'], best_params['l2'])
final_cb = [
    EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_accuracy', patience=6, factor=0.5, verbose=1),
]

history = final_model.fit(
    X_tr_all, y_tr_all,
    validation_data=(X_te, y_te),
    epochs=200, batch_size=best_params['batch_size'], verbose=1,
    class_weight=class_weight_dict,
    callbacks=final_cb
)

# =============================
# ì €ì¥ ë° ë³´ê³ ì„œ ì‚°ì¶œë¬¼
# =============================
model_path = os.path.join(SAVE_DIR, 'gesture_model.h5')
final_model.save(model_path)

with open(os.path.join(SAVE_DIR, 'label_map.json'), 'w', encoding='utf-8') as f:
    json.dump(label_list, f, ensure_ascii=False, indent=2)

# í•™ìŠµ ì´ë ¥ CSV/ê·¸ë˜í”„
hist_df = pd.DataFrame(history.history)
hist_df.to_csv(os.path.join(SAVE_DIR, 'history.csv'), index=False, encoding='utf-8-sig')

plt.figure()
plt.plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„')
plt.plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„')
plt.title('ì •í™•ë„ ë³€í™” (CNN)')
plt.xlabel('Epoch')
plt.ylabel('ì •í™•ë„')
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, 'accuracy_curve.png'), dpi=150)
plt.close()

plt.figure()
plt.plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤')
plt.plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤')
plt.title('ì†ì‹¤ ë³€í™” (CNN)')
plt.xlabel('Epoch')
plt.ylabel('ì†ì‹¤')
plt.legend()
plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, 'loss_curve.png'), dpi=150)
plt.close()

# í‰ê°€: í˜¼ë™í–‰ë ¬/ë¦¬í¬íŠ¸/ì˜ˆì¸¡ í™•ë¥  ì €ì¥ (ë³´ê³ ì„œìš©)
loss, acc = final_model.evaluate(X_te, y_te, verbose=0)
print(f"\n[FINAL CNN] í…ŒìŠ¤íŠ¸ ì •í™•ë„: {acc:.4f}")

probs = final_model.predict(X_te, verbose=0)
y_true_idx = np.argmax(y_te, axis=1)
y_pred_idx = np.argmax(probs, axis=1)

cm = confusion_matrix(y_true_idx, y_pred_idx, labels=list(range(len(label_list))))
cm_df = pd.DataFrame(cm, index=label_list, columns=label_list)
cm_df.to_csv(os.path.join(SAVE_DIR, 'confusion_matrix.csv'), encoding='utf-8-sig')

plt.figure(figsize=(8, 7))
plt.imshow(cm, interpolation='nearest')
plt.title('Confusion Matrix (CNN)')
plt.colorbar()
plt.xticks(ticks=np.arange(len(label_list)), labels=label_list, rotation=90)
plt.yticks(ticks=np.arange(len(label_list)), labels=label_list)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.savefig(os.path.join(SAVE_DIR, 'confusion_matrix.png'), dpi=160)
plt.close()

report_dict = classification_report(y_true_idx, y_pred_idx, target_names=label_list, output_dict=True, zero_division=0)
report_df = pd.DataFrame(report_dict).transpose()
report_df.to_csv(os.path.join(SAVE_DIR, 'classification_report.csv'), encoding='utf-8-sig')

result_rows = []
for i in range(len(y_te)):
    t = int(y_true_idx[i])
    p = int(y_pred_idx[i])
    conf = float(probs[i, p])
    result_rows.append({
        'true_index': t,
        'true_label': label_list[t],
        'pred_index': p,
        'pred_label': label_list[p],
        'confidence': conf
    })
results_df = pd.DataFrame(result_rows)
results_df.to_csv(os.path.join(SAVE_DIR, 'test_predictions.csv'), index=False, encoding='utf-8-sig')

summary = {
    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'data_path': DATA_PATH,
    'save_dir': SAVE_DIR,
    'required_frames': REQUIRED_FRAMES,
    'expected_len': EXPECTED_LEN,
    'num_classes': len(label_list),
    'classes': label_list,
    'train_samples': int(len(X_tr_all)),
    'test_samples': int(len(X_te)),
    'val_split_within_train': 0.15,
    'class_weights': class_weight_dict,
    'bayesian_optimization': {
        'enabled': bool(BO_ENABLE and HAS_BO),
        'init_points': BO_INIT,
        'n_iter': BO_ITER,
        'best_params': best_params
    },
    'final_test_accuracy': float(acc)
}
with open(os.path.join(SAVE_DIR, 'summary.json'), 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

print("\nâœ… ì‚°ì¶œë¬¼ ì €ì¥ ëª©ë¡")
for fn in [
    'gesture_model.h5',
    'label_map.json',
    'history.csv',
    'accuracy_curve.png',
    'loss_curve.png',
    'label_distribution.csv',
    'label_distribution.png',
    'confusion_matrix.csv',
    'confusion_matrix.png',
    'classification_report.csv',
    'test_predictions.csv',
    'best_params.json',
    'summary.json'
]:
    print(' -', os.path.join(SAVE_DIR, fn))

print("\nğŸ¯ ì™„ë£Œ: (CNN) ë³´ê³ ì„œìš© ê·¸ë˜í”„ì™€ CSVê°€ SAVE_DIRì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")