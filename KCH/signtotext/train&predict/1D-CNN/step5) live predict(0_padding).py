import cv2
import mediapipe as mp
import numpy as np
import json
import time
from tensorflow.keras.models import load_model
from PIL import ImageFont, ImageDraw, Image  # í•œê¸€ ì¶œë ¥ìš©

# ===== í•œê¸€ í…ìŠ¤íŠ¸ ì¶œë ¥ í•¨ìˆ˜ =====
def draw_text_korean(frame, text, position, font_size=30, color=(255,255,255)):
    font_path = "C:/Windows/Fonts/malgun.ttf"  # í•œê¸€ í°íŠ¸ ê²½ë¡œ
    font = ImageFont.truetype(font_path, font_size)
    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# ===== ëª¨ë¸ ë° ë¼ë²¨ ë¡œë”© =====
MODEL_PATH = r"C:\SoftwareEdu2025\project\Hand_Sound\KCH\signtotext\models\test3\frame_to_gloss_v0.h5"
LABEL_PATH = r"C:\SoftwareEdu2025\project\Hand_Sound\KCH\signtotext\models\test3\frame_to_gloss_v0.json"

model = load_model(MODEL_PATH, compile=False)
with open(LABEL_PATH, "r", encoding="utf-8") as f:
    label_list = json.load(f)
label_map = {i: label for i, label in enumerate(label_list)}

# ===== MediaPipe ì´ˆê¸°í™” =====
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils
holistic = mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7)

# ===== ì„¤ì • =====
POSE_SKIP_INDEXES = set(range(17, 33))  # í•˜ì²´ ì œì™¸
EXPECTED_KEYPOINTS = 194
BUFFER_SIZE = 10  # â† train í”„ë ˆì„ ìˆ˜ì™€ ì¼ì¹˜
CONFIDENCE_THRESHOLD = 0.98
STABLE_THRESHOLD = 3
RESET_INTERVAL = 5.0
MAX_PADDING_RATIO = 0.5
MIN_VALID_FRAMES = 6
MIN_INTERVAL_BETWEEN_SAME_WORD = 1.0  # ì´ˆ ë‹¨ìœ„

# ===== ìƒíƒœ ë³€ìˆ˜ =====
frame_buffer = []
last_stable_label = None
stable_count = 0
output_sentence = []
last_add_time = time.time()

cap = cv2.VideoCapture(0)

def extract_landmarks(landmarks, dims=3, skip=None):
    result = []
    if landmarks:
        for i, lm in enumerate(landmarks.landmark):
            if skip and i in skip:
                continue
            coords = [lm.x, lm.y, lm.z]
            if dims == 4:
                coords.append(getattr(lm, 'visibility', 0.0))
            result.extend(coords)
    return result

print("ğŸ” ì‹¤ì‹œê°„ ìˆ˜ì–´ ì˜ˆì¸¡ ì‹œì‘ (0íŒ¨ë”© ê¸°ë°˜, 'q'ë¡œ ì¢…ë£Œ)")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        continue

    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = holistic.process(image_rgb)
    display_frame = frame.copy()

    # ğŸ”¹ ì†ì´ ëª¨ë‘ ì—†ëŠ” ê²½ìš°
    if results.left_hand_landmarks is None and results.right_hand_landmarks is None:
        frame_buffer.clear()
        stable_count = 0
        last_stable_label = None
        predicted_label = "none"
        confidence = 0.0
        print(f"[SKIP] ì–‘ì† ì—†ìŒ â†’ None ì¶œë ¥")
        display_frame = draw_text_korean(display_frame, "ì—†ìŒ (0.00)", (10, 30), 32, (0, 0, 255))
        cv2.imshow("ì‹¤ì‹œê°„ ìˆ˜ì–´ ì˜ˆì¸¡", display_frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            print("â ì¢…ë£Œ")
            break
        continue

    # ğŸ”¹ ëœë“œë§ˆí¬ ì¶”ì¶œ
    lh = extract_landmarks(results.left_hand_landmarks, dims=3)
    rh = extract_landmarks(results.right_hand_landmarks, dims=3)
    pose = extract_landmarks(results.pose_landmarks, dims=4, skip=POSE_SKIP_INDEXES)
    keypoints = lh + rh + pose

    # ğŸ”¹ 0íŒ¨ë”©
    if len(keypoints) < EXPECTED_KEYPOINTS:
        keypoints += [0.0] * (EXPECTED_KEYPOINTS - len(keypoints))
    elif len(keypoints) > EXPECTED_KEYPOINTS:
        keypoints = keypoints[:EXPECTED_KEYPOINTS]

    # ğŸ”¹ ì…ë ¥ ë¬´íš¨ íŒë‹¨
    zero_ratio = keypoints.count(0.0) / EXPECTED_KEYPOINTS
    if zero_ratio > MAX_PADDING_RATIO:
        frame_buffer.clear()
        stable_count = 0
        last_stable_label = None
        display_frame = draw_text_korean(display_frame, "ë¬´íš¨ (0.00)", (10, 30), 32, (0, 0, 255))
        cv2.imshow("ì‹¤ì‹œê°„ ìˆ˜ì–´ ì˜ˆì¸¡", display_frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            print("â ì¢…ë£Œ")
            break
        continue

    # ğŸ”¹ í”„ë ˆì„ ë²„í¼
    frame_buffer.append(keypoints)
    if len(frame_buffer) > BUFFER_SIZE:
        frame_buffer.pop(0)

    predicted_label = "none"
    confidence = 0.0

    # ğŸ”¹ ì˜ˆì¸¡ ì¡°ê±´ ì¶©ì¡± ì‹œ
    if len(frame_buffer) == BUFFER_SIZE:
        input_data = np.array(frame_buffer)
        max_abs = np.max(np.abs(input_data))
        if max_abs > 0:
            input_data = input_data / max_abs
        input_data = np.expand_dims(input_data, axis=0)

        prediction = model.predict(input_data, verbose=0)
        pred_idx = int(np.argmax(prediction))
        confidence = float(np.max(prediction))
        predicted_label = label_map.get(pred_idx, "none") if confidence >= CONFIDENCE_THRESHOLD else "none"

        # ğŸ”¹ ë¬¸ì¥ ëˆ„ì  íŒë‹¨
        if predicted_label != "none":
            if predicted_label == last_stable_label:
                stable_count += 1
            else:
                last_stable_label = predicted_label
                stable_count = 1

            if stable_count == STABLE_THRESHOLD:
                if not output_sentence or predicted_label != output_sentence[-1]:
                    if time.time() - last_add_time > MIN_INTERVAL_BETWEEN_SAME_WORD:
                        output_sentence.append(predicted_label)
                        last_add_time = time.time()
                        if len(output_sentence) > 10:
                            output_sentence = output_sentence[-10:]
                print(f"[RUN] ì˜ˆì¸¡: {predicted_label}, ì •í™•ë„: {confidence:.2f}, ë¬¸ì¥: {' '.join(output_sentence)}")
        else:
            print(f"[NONE] ì˜ˆì¸¡: ì—†ìŒ, ì •í™•ë„: {confidence:.2f}")
            stable_count = 0
            last_stable_label = None

    # ğŸ”¹ ì‹œê°„ ì´ˆê³¼ ì‹œ ë¬¸ì¥ ì´ˆê¸°í™”
    if output_sentence and (time.time() - last_add_time > RESET_INTERVAL):
        output_sentence = []
        print(f"[RESET] {RESET_INTERVAL}ì´ˆ ê²½ê³¼ë¡œ ë¬¸ì¥ ì´ˆê¸°í™”")

    # ğŸ”¹ ì˜ˆì¸¡ ê²°ê³¼ ë° ëˆ„ì  ë¬¸ì¥ ì‹œê°í™” (í•œê¸€)
    label_text = "ì—†ìŒ" if predicted_label == "none" else predicted_label
    display_frame = draw_text_korean(display_frame, f'{label_text} ({confidence:.2f})', (10, 30),
                                     font_size=32, color=(0,255,0) if predicted_label != "none" else (0,0,255))
    display_frame = draw_text_korean(display_frame, ' '.join(output_sentence), (10, 70),
                                     font_size=28, color=(255,255,255))

    # ğŸ”¹ ëœë“œë§ˆí¬ ì‹œê°í™”
    mp_drawing.draw_landmarks(display_frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
    mp_drawing.draw_landmarks(display_frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    mp_drawing.draw_landmarks(display_frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

    cv2.imshow("ì‹¤ì‹œê°„ ìˆ˜ì–´ ì˜ˆì¸¡", display_frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        print("â ì¢…ë£Œ")
        break

cap.release()
cv2.destroyAllWindows()