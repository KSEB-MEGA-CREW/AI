import cv2
import mediapipe as mp
import numpy as np
import json
import time
from tensorflow.keras.models import load_model
from PIL import ImageFont, ImageDraw, Image  # í•œê¸€ ì¶œë ¥ìš©
from collections import Counter, defaultdict

# ===== í•œê¸€ í…ìŠ¤íŠ¸ ì¶œë ¥ í•¨ìˆ˜ =====
def draw_text_korean(frame, text, position, font_size=30, color=(255,255,255)):
    font_path = "C:/Windows/Fonts/malgun.ttf"  # í•œê¸€ í°íŠ¸ ê²½ë¡œ
    font = ImageFont.truetype(font_path, font_size)
    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# ===== ëª¨ë¸ ë° ë¼ë²¨ ë¡œë”© =====
MODEL_PATH = r"C:\Users\cksgu\Desktop\ê¹€ì°¬í˜\2025\KSEB\í”„ë¡œì íŠ¸\v5_cnn\gesture_model.h5"
LABEL_PATH = r"C:\Users\cksgu\Desktop\ê¹€ì°¬í˜\2025\KSEB\í”„ë¡œì íŠ¸\v5_cnn\label_map.json"

model = load_model(MODEL_PATH, compile=False)
with open(LABEL_PATH, "r", encoding="utf-8") as f:
    label_list = json.load(f)
label_map = {i: label for i, label in enumerate(label_list)}

# ===== MediaPipe ì´ˆê¸°í™” =====
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils
holistic = mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.7)

# ===== ì„¤ì • =====
POSE_SKIP_INDEXES = set(range(17, 33))  # í•˜ì²´ ì œì™¸
EXPECTED_KEYPOINTS = 194
BUFFER_SIZE = 10  # â† train í”„ë ˆì„ ìˆ˜ì™€ ì¼ì¹˜
CONFIDENCE_THRESHOLD = 0.96
STABLE_THRESHOLD = 2
RESET_INTERVAL = 5.0
MAX_PADDING_RATIO = 0.5
MIN_VALID_FRAMES = 6
MIN_INTERVAL_BETWEEN_SAME_WORD = 1.0  # ì´ˆ ë‹¨ìœ„

# ===== ìƒíƒœ ë³€ìˆ˜ =====
frame_buffer = []          # ì‹¤ì‹œê°„ ì˜ˆì¸¡ìš© ìˆœí™˜ ë²„í¼
last_stable_label = None
stable_count = 0
output_sentence = []
last_add_time = time.time()

# ë…¹í™” ê´€ë ¨
recording = False
record_buffer = []         # ë…¹í™” êµ¬ê°„ì˜ í”„ë ˆì„(í‚¤í¬ì¸íŠ¸) ì €ì¥
last_record_result = None  # {'text': str, 'expire': timestamp}

cap = cv2.VideoCapture(0)

def extract_landmarks(landmarks, dims=3, skip=None):
    result = []
    if landmarks:
        for i, lm in enumerate(landmarks.landmark):
            if skip and i in skip:
                continue
            coords = [lm.x, lm.y, lm.z]
            if dims == 4:
                coords.append(getattr(lm, 'visibility', 0.0))
            result.extend(coords)
    return result

def preprocess_keypoints(lh, rh, pose):
    keypoints = lh + rh + pose
    # 0íŒ¨ë”©/ìë¥´ê¸°
    if len(keypoints) < EXPECTED_KEYPOINTS:
        keypoints += [0.0] * (EXPECTED_KEYPOINTS - len(keypoints))
    elif len(keypoints) > EXPECTED_KEYPOINTS:
        keypoints = keypoints[:EXPECTED_KEYPOINTS]
    return keypoints

def is_valid_frame(keypoints):
    zero_ratio = keypoints.count(0.0) / EXPECTED_KEYPOINTS
    return zero_ratio <= MAX_PADDING_RATIO

def windowed_predictions(frames, buffer_size, threshold):
    """
    frames: [ [EXPECTED_KEYPOINTS], ... ]  (ê¸¸ì´ ê°€ë³€)
    buffer_size: ìœˆë„ìš° í¬ê¸° (ëª¨ë¸ ì…ë ¥ í”„ë ˆì„ ìˆ˜)
    return:
      preds: [("ë¼ë²¨" ë˜ëŠ” "none", conf), ...]  # stride=1
    """
    preds = []
    n = len(frames)
    if n == 0:
        return preds

    # ê¸¸ì´ê°€ ì§§ìœ¼ë©´ íŒ¨ë”©í•´ì„œ 1íšŒ ì˜ˆì¸¡
    if n < buffer_size:
        window = frames + [[0.0]*EXPECTED_KEYPOINTS for _ in range(buffer_size - n)]
        window = np.array(window, dtype=np.float32)
        max_abs = np.max(np.abs(window))
        if max_abs > 0:
            window = window / max_abs
        window = np.expand_dims(window, axis=0)
        p = model.predict(window, verbose=0)
        idx = int(np.argmax(p))
        conf = float(np.max(p))
        label = label_map.get(idx, "none") if conf >= threshold else "none"
        preds.append((label, conf))
        return preds

    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° stride=1
    for s in range(0, n - buffer_size + 1):
        window = np.array(frames[s:s + buffer_size], dtype=np.float32)
        max_abs = np.max(np.abs(window))
        if max_abs > 0:
            window = window / max_abs
        window = np.expand_dims(window, axis=0)
        p = model.predict(window, verbose=0)
        idx = int(np.argmax(p))
        conf = float(np.max(p))
        label = label_map.get(idx, "none") if conf >= threshold else "none"
        preds.append((label, conf))
    return preds

def summarize_predictions(preds):
    """
    preds: [ (label, conf), ... ]
    return:
      majority_label, majority_conf_mean,
      seq_compact (ì—°ì† ë™ì¼ ë¼ë²¨ ì••ì¶•, 'none' ì œê±°) as list[str],
      counts_dict (ë¼ë²¨ë³„ ê°œìˆ˜, 'none' í¬í•¨)
    """
    if not preds:
        return "none", 0.0, [], {}

    counts = Counter([lab for lab, _ in preds])
    # ìµœë‹¤ ë¼ë²¨(ê°€ëŠ¥í•˜ë©´ 'none' ì œì™¸) ì„ íƒ
    if len(counts) > 1 and 'none' in counts:
        # none ì œì™¸í•œ ê²ƒ ì¤‘ ìµœë‹¤
        non_none_counts = {k: v for k, v in counts.items() if k != 'none'}
        if non_none_counts:
            majority_label = max(non_none_counts, key=non_none_counts.get)
        else:
            majority_label = 'none'
    else:
        majority_label = counts.most_common(1)[0][0]

    # í•´ë‹¹ ë¼ë²¨ì˜ í‰ê·  confidence
    confs = [conf for lab, conf in preds if lab == majority_label]
    majority_conf_mean = float(np.mean(confs)) if confs else 0.0

    # ì—°ì† ë™ì¼ ë¼ë²¨ ì••ì¶• (none ì œê±°)
    seq_compact = []
    last = None
    for lab, _ in preds:
        if lab == 'none':
            last = lab
            continue
        if lab != last:
            seq_compact.append(lab)
        last = lab

    return majority_label, majority_conf_mean, seq_compact, dict(counts)

print("ğŸ” ì‹¤ì‹œê°„ ìˆ˜ì–´ ì˜ˆì¸¡ ì‹œì‘ (0íŒ¨ë”© ê¸°ë°˜, 'q' ì¢…ë£Œ / 'r' ë…¹í™” í† ê¸€ / 'c' ë…¹í™” ì·¨ì†Œ)")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        continue

    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = holistic.process(image_rgb)
    display_frame = frame.copy()

    # ğŸ”¹ ëœë“œë§ˆí¬ ì¶”ì¶œ
    lh = extract_landmarks(results.left_hand_landmarks, dims=3)
    rh = extract_landmarks(results.right_hand_landmarks, dims=3)
    pose = extract_landmarks(results.pose_landmarks, dims=4, skip=POSE_SKIP_INDEXES)
    keypoints = preprocess_keypoints(lh, rh, pose)
    valid = is_valid_frame(keypoints)

    # ğŸ”¹ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë¡œì§ (ê¸°ì¡´ ìœ ì§€)
    predicted_label = "none"
    confidence = 0.0

    # ì†ì´ ëª¨ë‘ ì—†ëŠ” ê²½ìš°: ì‹¤ì‹œê°„ ë²„í¼/ìƒíƒœ ë¦¬ì…‹
    if results.left_hand_landmarks is None and results.right_hand_landmarks is None:
        frame_buffer.clear()
        stable_count = 0
        last_stable_label = None
        predicted_label = "none"
        confidence = 0.0
        display_frame = draw_text_korean(display_frame, "ì—†ìŒ (0.00)", (10, 30), 32, (0, 0, 255))
    else:
        # ìœ íš¨ í”„ë ˆì„ë§Œ ì‹¤ì‹œê°„ ë²„í¼ì— ì‚¬ìš©
        if valid:
            frame_buffer.append(keypoints)
            if len(frame_buffer) > BUFFER_SIZE:
                frame_buffer.pop(0)

        if len(frame_buffer) == BUFFER_SIZE:
            input_data = np.array(frame_buffer, dtype=np.float32)
            max_abs = np.max(np.abs(input_data))
            if max_abs > 0:
                input_data = input_data / max_abs
            input_data = np.expand_dims(input_data, axis=0)

            prediction = model.predict(input_data, verbose=0)
            pred_idx = int(np.argmax(prediction))
            confidence = float(np.max(prediction))
            predicted_label = label_map.get(pred_idx, "none") if confidence >= CONFIDENCE_THRESHOLD else "none"

            # ë¬¸ì¥ ëˆ„ì 
            if predicted_label != "none":
                if predicted_label == last_stable_label:
                    stable_count += 1
                else:
                    last_stable_label = predicted_label
                    stable_count = 1

                if stable_count == STABLE_THRESHOLD:
                    if not output_sentence or predicted_label != output_sentence[-1]:
                        if time.time() - last_add_time > MIN_INTERVAL_BETWEEN_SAME_WORD:
                            output_sentence.append(predicted_label)
                            last_add_time = time.time()
                            if len(output_sentence) > 10:
                                output_sentence = output_sentence[-10:]
            else:
                stable_count = 0
                last_stable_label = None

        # ì‹œê°„ ì´ˆê³¼ ì‹œ ë¬¸ì¥ ì´ˆê¸°í™”
        if output_sentence and (time.time() - last_add_time > RESET_INTERVAL):
            output_sentence = []

        # ì‹œê°í™”
        label_text = "ì—†ìŒ" if predicted_label == "none" else predicted_label
        display_frame = draw_text_korean(display_frame, f'{label_text} ({confidence:.2f})', (10, 30),
                                         font_size=32, color=(0,255,0) if predicted_label != "none" else (0,0,255))
        display_frame = draw_text_korean(display_frame, ' '.join(output_sentence), (10, 70),
                                         font_size=28, color=(255,255,255))

    # ğŸ”¹ ë…¹í™” ì¤‘ì´ë©´ ìœ íš¨ í”„ë ˆì„ë§Œ ì €ì¥
    if recording and valid:
        record_buffer.append(keypoints)

    # ğŸ”¹ ëœë“œë§ˆí¬ ì‹œê°í™”
    mp_drawing.draw_landmarks(display_frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
    mp_drawing.draw_landmarks(display_frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    mp_drawing.draw_landmarks(display_frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

    # ğŸ”¹ ë…¹í™” ìƒíƒœ í‘œì‹œ
    if recording:
        display_frame = draw_text_korean(display_frame, f'â— REC  í”„ë ˆì„: {len(record_buffer)}',
                                         (10, 110), font_size=28, color=(0,0,255))

    # ğŸ”¹ ìµœê·¼ ë…¹í™” ê²°ê³¼ ì˜¤ë²„ë ˆì´ (3ì´ˆ í‘œì‹œ)
    if last_record_result and time.time() < last_record_result['expire']:
        display_frame = draw_text_korean(display_frame, last_record_result['text'],
                                         (10, 150), font_size=26, color=(255, 215, 0))

    cv2.imshow("ì‹¤ì‹œê°„ ìˆ˜ì–´ ì˜ˆì¸¡ + ë…¹í™” í…ŒìŠ¤íŠ¸", display_frame)

    # ğŸ”¹ í‚¤ ì…ë ¥ ì²˜ë¦¬
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        print("â ì¢…ë£Œ")
        break
    elif key == ord('r'):
        # ë…¹í™” í† ê¸€
        recording = not recording
        if recording:
            record_buffer = []
            print("âº ë…¹í™” ì‹œì‘")
            last_record_result = None
        else:
            # ë…¹í™” ì¢…ë£Œ â†’ ë¶„ì„
            print(f"â¹ ë…¹í™” ì¤‘ì§€. ìˆ˜ì§‘ í”„ë ˆì„: {len(record_buffer)}")
            preds = windowed_predictions(record_buffer, BUFFER_SIZE, CONFIDENCE_THRESHOLD)
            majority_label, majority_conf, seq_compact, counts = summarize_predictions(preds)

            # ì½˜ì†” ì¶œë ¥
            print(f"[ë…¹í™” ê²°ê³¼] í”„ë ˆì„={len(record_buffer)} ìœˆë„ìš°={len(preds)}")
            print(f" - ìµœë‹¤ ë¼ë²¨: {majority_label} (avg conf: {majority_conf:.2f})")
            print(f" - ì—°ì† ì‹œí€€ìŠ¤: {' '.join(seq_compact) if seq_compact else '(ì—†ìŒ)'}")
            print(f" - ë¼ë²¨ ì¹´ìš´íŠ¸: {counts}")

            # í™”ë©´ ì˜¤ë²„ë ˆì´ìš© ë©”ì‹œì§€ (3ì´ˆ ë…¸ì¶œ)
            overlay_text = f"[REC ê²°ê³¼] {majority_label} ({majority_conf:.2f}) | ì‹œí€€ìŠ¤: {' '.join(seq_compact) if seq_compact else '(ì—†ìŒ)'}"
            last_record_result = {'text': overlay_text, 'expire': time.time() + 3.0}
    elif key == ord('c'):
        # ë…¹í™” ì·¨ì†Œ
        if recording or record_buffer:
            recording = False
            record_buffer = []
            last_record_result = {'text': "[REC] ì·¨ì†Œë¨", 'expire': time.time() + 2.0}
            print("ğŸ—‘ ë…¹í™” ì·¨ì†Œ ë° ë²„í¼ ì‚­ì œ")

cap.release()
holistic.close()
cv2.destroyAllWindows()